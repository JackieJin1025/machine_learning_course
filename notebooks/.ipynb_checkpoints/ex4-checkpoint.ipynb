{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import scipy.optimize as opt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = \"/Users/Jackie/Dropbox/Work/machine_learning/hw/machine-learning-ex4/ex4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = loadmat(os.path.join(base_dir, 'ex4data1'))\n",
    "data_weights = loadmat(os.path.join(base_dir, 'ex4weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "Theta1 = data_weights['Theta1']\n",
    "Theta2 = data_weights['Theta2']\n",
    "print(Theta1.shape, Theta2.shape)\n",
    "\n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "nn_parms = np.concatenate([Theta1.flatten(), Theta2.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "y = y[:, 0]\n",
    "m = X.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):    \n",
    "    res = 1 / (1+ np.exp(-z))\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X, theta1, theta2): \n",
    "    \"\"\"\n",
    "        feed forward propagation\n",
    "        X is orignal data with m obs, and n features \n",
    "        \n",
    "        theta output_size * (input_size + 1)\n",
    "        X*theta.T --> next layer \n",
    "        \n",
    "    \"\"\"\n",
    "    # add constant feature    \n",
    "    a1 = np.insert(X, 0, values=np.ones(X.shape[0]), axis=1)    \n",
    "    z2 = a1.dot(theta1.T)\n",
    "    a2 = sigmoid(z2) #\n",
    "    \n",
    "   # add constant feature     \n",
    "    a2 = np.insert(a2, 0, values=np.ones(a2.shape[0]), axis=1)    \n",
    "    z3 =a2.dot(theta2.T)\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    return a1, z2, a2, z3, a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nnCost(params, input_size, hidden_size, num_labels, X, y, penalty=0):\n",
    "    \"\"\"\n",
    "        params are 1D array including theta1 and theta2\n",
    "        input_size: size of 1st layer\n",
    "        hidden_size: size of second layer\n",
    "        num_labels: size of third/last layer\n",
    "        X: m *input_size 2D array (m observations)\n",
    "        y: 1D array\n",
    "    \"\"\"\n",
    "    # number of obs \n",
    "    m = X.shape[0]    \n",
    "    \n",
    "    # convert y to binary 2d array with num_labels cols \n",
    "    y_list = []\n",
    "    for i in range(1, num_labels+1):\n",
    "        y_list.append(y == i ) \n",
    "    y = np.vstack(y_list).T\n",
    "    \n",
    "    \n",
    "    # theta is in shape (output_size , input_size +1)\n",
    "    # reshape the parameter array into parameter matrices for each layer\n",
    "    theta1 = np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1)))\n",
    "    theta2 = np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1)))\n",
    "        \n",
    "    \n",
    "    # run the feed-forward pass\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)    \n",
    "    theta_sum = np.sum(np.power(theta1[:, 1:],2)) + np.sum(np.power(theta2[:, 1:],2)) \n",
    "        \n",
    "    \n",
    "    # compute the cost        \n",
    "    d = - (y*np.log(h) + (1-y)*np.log(1-h)) #piecewise\n",
    "    J = np.sum(d) / m  + theta_sum * penalty /(2*m)           \n",
    "    return J    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2876291651613189\n"
     ]
    }
   ],
   "source": [
    "J = nnCost(nn_parms, input_layer_size, hidden_layer_size, num_labels, X, y )\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38376985909092365\n"
     ]
    }
   ],
   "source": [
    "J = nnCost(nn_parms, input_layer_size, hidden_layer_size, num_labels, X, y, penalty=1)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(nRow, nCol):\n",
    "    e_init = 0.12\n",
    "    theta = np.random.uniform(e_init, -e_init, (nRow, nCol))    \n",
    "    return theta    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n"
     ]
    }
   ],
   "source": [
    "def sigmoidGradient(Z):\n",
    "    a = 1/(1+ np.exp(-Z))\n",
    "    g = a * (1-a)\n",
    "    return g \n",
    "\n",
    "print(sigmoidGradient(np.array([-1, -0.5, 0, 0.5, 1])) )\n",
    "\n",
    "def backprop(params, input_size, hidden_size, num_labels, X, y, penalty=0):\n",
    "    \"\"\"\n",
    "        params are 1D array after flattening theta1 and theta2\n",
    "        input_size: size of 1st layer\n",
    "        hidden_size: size of second layer\n",
    "        num_labels: size of third/last layer\n",
    "        X: m *input_size 2D array (m observations)\n",
    "        y: 1D array\n",
    "    \"\"\"\n",
    "    # number of obs \n",
    "    m = X.shape[0]    \n",
    "    \n",
    "    # convert y to binary 2d array with num_labels cols \n",
    "    y_list = []\n",
    "    for i in range(1, num_labels+1):\n",
    "        y_list.append(y == i ) \n",
    "    y = np.vstack(y_list).T\n",
    "    \n",
    "    \n",
    "    # theta is in shape (output_size , input_size +1)\n",
    "    # reshape the parameter array into parameter matrices for each layer\n",
    "    # theta1 (25, 401)\n",
    "    # theta2 (10, 26)\n",
    "    theta1 = np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1)))\n",
    "    theta2 = np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1)))\n",
    "        \n",
    "    \n",
    "    # run the feed-forward pass\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)    \n",
    "    theta_sum = np.sum(np.power(theta1[:, 1:],2)) + np.sum(np.power(theta2[:, 1:],2)) \n",
    "    \n",
    "    \n",
    "    # perform backpropagation \n",
    "    # d3, d2 are errors in layer 3, layer2\n",
    "    d3 = h - y  #(5000, 10)            \n",
    "    z2 = np.insert(z2, 0, values=np.ones(z2.shape[0]), axis=1) # (5000, 26)       \n",
    "    d2 = d3.dot(theta2) * sigmoidGradient(z2) # (5000, 26)\n",
    "    \n",
    "    # complicated math proof --> grad_l(i,j) = a_l(j) * d_l+1(i) \n",
    "    grad1 = d2[:, 1:].T.dot(a1)/m  # (25, 5000) * (5000, 401) -->  (25, 401)    \n",
    "    grad2 = d3.T.dot(a2) /m  #  (10, 5000) * (5000, 25)  -->  (10, 26)\n",
    "        \n",
    "    \n",
    "    #add the gradient regularization term \n",
    "    grad1[:, 1:] = grad1[:, 1:] + theta1[:, 1:] * penalty / m \n",
    "    grad2[:, 1:] = grad2[:, 1:] + theta2[:, 1:] * penalty / m \n",
    "    \n",
    "    #flatten grads \n",
    "    grad = np.concatenate((np.ravel(grad1), np.ravel(grad2)))\n",
    "    # compute the cost        \n",
    "    d = - (y*np.log(h) + (1-y)*np.log(1-h)) #piecewise\n",
    "    J = np.sum(d) / m  + theta_sum * penalty /(2*m)           \n",
    "    return J , grad   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeNumericalGradient(func, params, *args):\n",
    "    numgrad = np.zeros(params.shape)\n",
    "    perturb = np.zeros(params.shape)\n",
    "    e = 1e-4\n",
    "    for i in range(len(params)):\n",
    "        perturb[i] = e        \n",
    "        loss1, _ = func(params+perturb, *args)\n",
    "        loss2, _ = func(params-perturb, *args)\n",
    "        numgrad[i] = (loss1 - loss2)/ (2*e)\n",
    "        perturb[i] = 0\n",
    "        \n",
    "    return numgrad        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.18712766e-05 -2.11248326e-12  4.38829369e-13 ...  4.70513145e-05\n",
      " -5.01718610e-04  5.07825789e-04]\n"
     ]
    }
   ],
   "source": [
    "J, grad = backprop(nn_parms, input_layer_size, hidden_layer_size, num_labels, X, y, penalty=1)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38,)\n",
      "[-0.0024212  -0.02144534 -0.01197526 -0.01484874  0.01338965 -0.02062371\n",
      "  0.01687598 -0.01942262 -0.00672811  0.0207332  -0.01141122  0.01609178\n",
      " -0.00926134  0.01302662 -0.0048634  -0.01511669  0.00200163 -0.00408522\n",
      " -0.00910892  0.00783545  0.14019128  0.08829563  0.08254461  0.0728222\n",
      "  0.06373491  0.0819915   0.11163253  0.0739031   0.07385752  0.0609078\n",
      "  0.03497813  0.03664405  0.30465712  0.12264849  0.17509738  0.12969668\n",
      "  0.14971759  0.16377415]\n",
      "[-0.0024212  -0.02144534 -0.01197526 -0.01484874  0.01338965 -0.02062371\n",
      "  0.01687598 -0.01942262 -0.00672811  0.0207332  -0.01141122  0.01609178\n",
      " -0.00926134  0.01302662 -0.0048634  -0.01511669  0.00200163 -0.00408522\n",
      " -0.00910892  0.00783545  0.14019128  0.08829563  0.08254461  0.0728222\n",
      "  0.06373491  0.0819915   0.11163253  0.0739031   0.07385752  0.0609078\n",
      "  0.03497813  0.03664405  0.30465712  0.12264849  0.17509738  0.12969668\n",
      "  0.14971759  0.16377415]\n",
      "3.70457013212677e-11\n"
     ]
    }
   ],
   "source": [
    "def checkNNGradients(penalty):\n",
    "    \"\"\"gradient checking\"\"\"\n",
    "    input_layer_size = 3\n",
    "    hidden_layer_size = 5\n",
    "    num_labels = 3\n",
    "    m = 5\n",
    "    theta1 = randInitializeWeights(hidden_layer_size, input_layer_size+1)\n",
    "    theta2 = randInitializeWeights(num_labels, hidden_layer_size+1)    \n",
    "    X  = randInitializeWeights(m, input_layer_size)\n",
    "    y = 1 + np.mod(range(m),num_labels)\n",
    "    nn_parms = np.concatenate((np.ravel(theta1), np.ravel(theta2)))\n",
    "    print(nn_parms.shape)\n",
    "    J, grad = backprop(nn_parms, input_layer_size, hidden_layer_size, num_labels, X, y, penalty)    \n",
    "    numgrad = computeNumericalGradient(backprop, nn_parms, input_layer_size, hidden_layer_size, num_labels, X, y, penalty)\n",
    "    print(grad)\n",
    "    print(numgrad)\n",
    "    \n",
    "    print(np.linalg.norm(grad-numgrad) / np.linalg.norm(grad+numgrad))\n",
    "checkNNGradients(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.23606797749979"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.array([0,1]) - np.array([1,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 0.7327504494128155\n",
      "     jac: array([ 1.16132693e-03,  2.72800451e-06, -1.15179714e-06, ...,\n",
      "       -2.86390063e-04, -8.32941329e-03, -3.66876349e-03])\n",
      " message: 'Max. number of function evaluations reached'\n",
      "    nfev: 50\n",
      "     nit: 10\n",
      "  status: 3\n",
      " success: False\n",
      "       x: array([ 0.14999375,  0.01364002, -0.00575899, ..., -2.34240129,\n",
      "        0.63045365,  0.4794079 ])\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "\n",
    "t1 = randInitializeWeights(hidden_layer_size, input_layer_size+1)\n",
    "t2 = randInitializeWeights(num_labels, hidden_layer_size+1)\n",
    "params = np.concatenate([t1.flatten(), t2.flatten()])\n",
    "\n",
    "penalty = 1\n",
    "\n",
    "# minimize the objective function\n",
    "fmin = opt.minimize(fun=backprop, x0=params, args=(input_layer_size, hidden_layer_size, num_labels, X, y, penalty), \n",
    "                method='TNC', jac=True, options={'maxiter': 50})\n",
    "print(fmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_prime_1 = np.reshape(fmin.x[:hidden_layer_size * (input_layer_size + 1)], (hidden_layer_size, (input_layer_size + 1)))\n",
    "theta_prime_2 = np.reshape(fmin.x[hidden_layer_size * (input_layer_size + 1):], (num_labels, (hidden_layer_size + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 26)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_prime_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9088\n"
     ]
    }
   ],
   "source": [
    "def predict(theta1, theta2, X):\n",
    "    _, _, _, _, h = forward_propagate(X, theta1, theta2)\n",
    "    y_pred = np.argmax(h, axis=1) + 1\n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict(theta_prime_1, theta_prime_2, X)\n",
    "accuracy = np.sum((y_pred == y))/ len(y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
