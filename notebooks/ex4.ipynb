{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import scipy.optimize as opt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = \"/Users/Jackie/Dropbox/Work/machine_learning/hw/machine-learning-ex4/ex4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = loadmat(os.path.join(base_dir, 'ex4data1'))\n",
    "data_weights = loadmat(os.path.join(base_dir, 'ex4weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "Theta1 = data_weights['Theta1']\n",
    "Theta2 = data_weights['Theta2']\n",
    "print(Theta1.shape, Theta2.shape)\n",
    "\n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "nn_parms = np.concatenate([Theta1.flatten(), Theta2.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "y = y[:, 0]\n",
    "m = X.shape[0]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):    \n",
    "    res = 1 / (1+ np.exp(-z))\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagate(X, theta1, theta2): \n",
    "    \"\"\"\n",
    "        feed forward propagation\n",
    "        X is orignal data with m obs, and n features \n",
    "        \n",
    "        theta output_size * (input_size + 1)\n",
    "        X*theta.T --> next layer \n",
    "        \n",
    "    \"\"\"\n",
    "    # add constant feature    \n",
    "    a1 = np.insert(X, 0, values=np.ones(X.shape[0]), axis=1)    \n",
    "    z2 = a1.dot(theta1.T)\n",
    "    a2 = sigmoid(z2) #\n",
    "    \n",
    "   # add constant feature     \n",
    "    a2 = np.insert(a2, 0, values=np.ones(a2.shape[0]), axis=1)    \n",
    "    z3 =a2.dot(theta2.T)\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    return a1, z2, a2, z3, a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nnCost(params, input_size, hidden_size, num_labels, X, y, penalty=0):\n",
    "    \"\"\"\n",
    "        params are 1D array including theta1 and theta2\n",
    "        input_size: size of 1st layer\n",
    "        hidden_size: size of second layer\n",
    "        num_labels: size of third/last layer\n",
    "        X: m *input_size 2D array (m observations)\n",
    "        y: 1D array\n",
    "    \"\"\"\n",
    "    # number of obs \n",
    "    m = X.shape[0]    \n",
    "    \n",
    "    # convert y to binary 2d array with num_labels cols \n",
    "    y_list = []\n",
    "    for i in range(1, num_labels+1):\n",
    "        y_list.append(y == i ) \n",
    "    y = np.vstack(y_list).T\n",
    "    \n",
    "    \n",
    "    # theta is in shape (output_size , input_size +1)\n",
    "    # reshape the parameter array into parameter matrices for each layer\n",
    "    theta1 = np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1)))\n",
    "    theta2 = np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1)))\n",
    "        \n",
    "    \n",
    "    # run the feed-forward pass\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)    \n",
    "    theta_sum = np.sum(np.power(theta1[:, 1:],2)) + np.sum(np.power(theta2[:, 1:],2)) \n",
    "        \n",
    "    \n",
    "    # compute the cost        \n",
    "    d = - (y*np.log(h) + (1-y)*np.log(1-h)) #piecewise\n",
    "    J = np.sum(d) / m  + theta_sum * penalty /(2*m)           \n",
    "    return J    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost function with penalty 0: 0.2876291651613189\n",
      "cost function with penalty 1: 0.38376985909092365\n"
     ]
    }
   ],
   "source": [
    "J = nnCost(nn_parms, input_layer_size, hidden_layer_size, num_labels, X, y, penalty=0)\n",
    "print(\"cost function with penalty 0:\", J)\n",
    "J = nnCost(nn_parms, input_layer_size, hidden_layer_size, num_labels, X, y, penalty=1)\n",
    "print(\"cost function with penalty 1:\", J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randInitializeWeights(nRow, nCol):\n",
    "    e_init = 0.12\n",
    "    theta = np.random.uniform(e_init, -e_init, (nRow, nCol))    \n",
    "    return theta    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n"
     ]
    }
   ],
   "source": [
    "def sigmoidGradient(Z):\n",
    "    a = 1/(1+ np.exp(-Z))\n",
    "    g = a * (1-a)\n",
    "    return g \n",
    "\n",
    "print(sigmoidGradient(np.array([-1, -0.5, 0, 0.5, 1])) )\n",
    "\n",
    "def backprop(params, input_size, hidden_size, num_labels, X, y, penalty=0):\n",
    "    \"\"\"\n",
    "        params are 1D array after flattening theta1 and theta2\n",
    "        input_size: size of 1st layer\n",
    "        hidden_size: size of second layer\n",
    "        num_labels: size of third/last layer\n",
    "        X: m *input_size 2D array (m observations)\n",
    "        y: 1D array\n",
    "    \"\"\"\n",
    "    # number of obs \n",
    "    m = X.shape[0]    \n",
    "    \n",
    "    # convert y to binary 2d array with num_labels cols \n",
    "    y_list = []\n",
    "    for i in range(1, num_labels+1):\n",
    "        y_list.append(y == i ) \n",
    "    y = np.vstack(y_list).T\n",
    "    \n",
    "    \n",
    "    # theta is in shape (output_size , input_size +1)\n",
    "    # reshape the parameter array into parameter matrices for each layer\n",
    "    # theta1 (25, 401)\n",
    "    # theta2 (10, 26)\n",
    "    theta1 = np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1)))\n",
    "    theta2 = np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1)))\n",
    "        \n",
    "    \n",
    "    # run the feed-forward pass\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)    \n",
    "    theta_sum = np.sum(np.power(theta1[:, 1:],2)) + np.sum(np.power(theta2[:, 1:],2)) \n",
    "    \n",
    "    \n",
    "    # perform backpropagation \n",
    "    # d3, d2 are errors in layer 3, layer2\n",
    "    d3 = h - y  #(5000, 10)            \n",
    "    z2 = np.insert(z2, 0, values=np.ones(z2.shape[0]), axis=1) # (5000, 26)       \n",
    "    d2 = d3.dot(theta2) * sigmoidGradient(z2) # (5000, 26)\n",
    "    \n",
    "    # complicated math proof --> grad_l(i,j) = a_l(j) * d_l+1(i) \n",
    "    grad2 = d3.T.dot(a2) /m  #  (10, 5000) * (5000, 25)  -->  (10, 26)\n",
    "    \n",
    "    grad1 = d2[:, 1:].T.dot(a1)/m  # (25, 5000) * (5000, 401) -->  (25, 401)    \n",
    "    \n",
    "        \n",
    "    \n",
    "    #add the gradient regularization term \n",
    "    grad1[:, 1:] = grad1[:, 1:] + theta1[:, 1:] * penalty / m \n",
    "    grad2[:, 1:] = grad2[:, 1:] + theta2[:, 1:] * penalty / m \n",
    "    \n",
    "    #flatten grads \n",
    "    grad = np.concatenate((np.ravel(grad1), np.ravel(grad2)))\n",
    "    # compute the cost        \n",
    "    d = - (y*np.log(h) + (1-y)*np.log(1-h)) #piecewise\n",
    "    J = np.sum(d) / m  + theta_sum * penalty /(2*m)           \n",
    "    return J , grad   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeNumericalGradient(func, params, *args):\n",
    "    numgrad = np.zeros(params.shape)\n",
    "    perturb = np.zeros(params.shape)\n",
    "    e = 1e-4\n",
    "    for i in range(len(params)):\n",
    "        perturb[i] = e        \n",
    "        loss1, _ = func(params+perturb, *args)\n",
    "        loss2, _ = func(params-perturb, *args)\n",
    "        numgrad[i] = (loss1 - loss2)/ (2*e)\n",
    "        perturb[i] = 0\n",
    "        \n",
    "    return numgrad        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.18712766e-05 -2.11248326e-12  4.38829369e-13 ...  4.70513145e-05\n",
      " -5.01718610e-04  5.07825789e-04]\n"
     ]
    }
   ],
   "source": [
    "J, grad = backprop(nn_parms, input_layer_size, hidden_layer_size, num_labels, X, y, penalty=1)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38,)\n",
      "[-0.00807759 -0.02267003 -0.01382068 -0.01715889  0.00365887  0.00929486\n",
      " -0.01247593 -0.00150642  0.00203073 -0.01890614 -0.02216543 -0.00101812\n",
      "  0.00965727 -0.00233986  0.01697357 -0.00473532  0.00467024 -0.00236324\n",
      " -0.00628784  0.00828812  0.05941649  0.03720971  0.00883356  0.01233656\n",
      "  0.02307268  0.02862346  0.10501174  0.02919758  0.07814001  0.0302043\n",
      "  0.07560734  0.0680216   0.34333312  0.15342453  0.18092263  0.18058884\n",
      "  0.1909293   0.17758094]\n",
      "[-0.00807759 -0.02267003 -0.01382068 -0.01715889  0.00365887  0.00929486\n",
      " -0.01247593 -0.00150642  0.00203073 -0.01890614 -0.02216543 -0.00101812\n",
      "  0.00965727 -0.00233986  0.01697357 -0.00473532  0.00467024 -0.00236324\n",
      " -0.00628784  0.00828812  0.05941649  0.03720971  0.00883356  0.01233656\n",
      "  0.02307268  0.02862346  0.10501174  0.02919758  0.07814001  0.0302043\n",
      "  0.07560734  0.0680216   0.34333312  0.15342453  0.18092263  0.18058884\n",
      "  0.1909293   0.17758094]\n",
      "4.8035109282725497e-11\n"
     ]
    }
   ],
   "source": [
    "def checkNNGradients(penalty):\n",
    "    \"\"\"gradient checking\"\"\"\n",
    "    input_layer_size = 3\n",
    "    hidden_layer_size = 5\n",
    "    num_labels = 3\n",
    "    m = 5\n",
    "    theta1 = randInitializeWeights(hidden_layer_size, input_layer_size+1)\n",
    "    theta2 = randInitializeWeights(num_labels, hidden_layer_size+1)    \n",
    "    X  = randInitializeWeights(m, input_layer_size)\n",
    "    y = 1 + np.mod(range(m),num_labels)\n",
    "    nn_parms = np.concatenate((np.ravel(theta1), np.ravel(theta2)))\n",
    "    print(nn_parms.shape)\n",
    "    J, grad = backprop(nn_parms, input_layer_size, hidden_layer_size, num_labels, X, y, penalty)    \n",
    "    numgrad = computeNumericalGradient(backprop, nn_parms, input_layer_size, hidden_layer_size, num_labels, X, y, penalty)\n",
    "    print(grad)\n",
    "    print(numgrad)\n",
    "    \n",
    "    print(np.linalg.norm(grad-numgrad) / np.linalg.norm(grad+numgrad))\n",
    "\n",
    "checkNNGradients(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 0.8291747208654137\n",
      "     jac: array([ 4.89373134e-03,  1.68054436e-05,  9.75643588e-07, ...,\n",
      "       -6.36766995e-03, -7.30806095e-04, -7.92794799e-03])\n",
      " message: 'Max. number of function evaluations reached'\n",
      "    nfev: 50\n",
      "     nit: 10\n",
      "  status: 3\n",
      " success: False\n",
      "       x: array([ 0.15632415,  0.08402722,  0.00487822, ..., -0.09090493,\n",
      "       -3.4259673 , -0.78276053])\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "t1 = randInitializeWeights(hidden_layer_size, input_layer_size+1)\n",
    "t2 = randInitializeWeights(num_labels, hidden_layer_size+1)\n",
    "params = np.concatenate([t1.flatten(), t2.flatten()])\n",
    "penalty = 1\n",
    "# minimize the objective function\n",
    "fmin = opt.minimize(fun=backprop, x0=params, args=(input_layer_size, hidden_layer_size, num_labels, X, y, penalty), \n",
    "                method='TNC', jac=True, options={'maxiter': 50})\n",
    "print(fmin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_prime_1 = np.reshape(fmin.x[:hidden_layer_size * (input_layer_size + 1)], (hidden_layer_size, (input_layer_size + 1)))\n",
    "theta_prime_2 = np.reshape(fmin.x[hidden_layer_size * (input_layer_size + 1):], (num_labels, (hidden_layer_size + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 26)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_prime_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN accuracy:  0.801\n"
     ]
    }
   ],
   "source": [
    "def predict(theta1, theta2, X):\n",
    "    _, _, _, _, h = forward_propagate(X, theta1, theta2)\n",
    "    y_pred = np.argmax(h, axis=1) + 1\n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict(theta_prime_1, theta_prime_2, X)\n",
    "accuracy = np.sum((y_pred == y))/ len(y_pred)\n",
    "print(\"NN accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
